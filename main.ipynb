{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "import matplotlib as plt\n",
    "import gensim\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import print_data_for_visualization, get_QA, to_csw\n",
    "#print_data_for_visualization()\n",
    "data_path = '.data/train-v2.0.json'\n",
    "csw_path = '.data/QA.txt'\n",
    "#Read in json data and build QA.txt run this line only 1st time\n",
    "#to_csw(get_QA(data_path=data_path), csw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pairs) = 86809\n",
      "Read 86809 sentence pairs\n",
      "Trimming pairs with sentence longer than MAX_LEN((10, 5))......\n",
      "trimmed to 23768 pairs\n",
      "[['when did beyonce start becoming popular ?', 'in the late 1990s'], ['in which decade did beyonce become famous ?', 'late 1990s'], ['what album made her a worldwide known artist ?', 'dangerously in love'], ['who managed the destiny s child group ?', 'mathew knowles'], ['when did beyonce rise to fame ?', 'late 1990s']]\n"
     ]
    }
   ],
   "source": [
    "from helper import get_pairs, filter_pairs\n",
    "data_path = os.path.join(os.getcwd(), '.data/QA.txt')\n",
    "#read in and normalize data\n",
    "pairs = get_pairs(data_path=data_path)\n",
    "print(f'len(pairs) = {len(pairs)}')\n",
    "#trim data with questions len > 10 and answers len >5\n",
    "max_question_lenght, max_answer_lenght = 10, 5\n",
    "MAX_LEN = (max_question_lenght, max_answer_lenght)\n",
    "pairs = filter_pairs(pairs, MAX_LEN)\n",
    "print(pairs[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populating vocabulary........\n",
      "Counted words: 25900\n"
     ]
    }
   ],
   "source": [
    "#Building Vocabulary\n",
    "from vocabulary import Vocab\n",
    "#initialize vocab object\n",
    "vocab = Vocab('SQuAD2.0')\n",
    "print('populating vocabulary........')\n",
    "for pair in pairs:\n",
    "    vocab.add_sentence(pair[0])\n",
    "    vocab.add_sentence(pair[1])\n",
    "print('Counted words: {}'.format(vocab.numb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........This is print from Voc class after call to trimm method.........\n",
      "keep_words 8190 / 25900 = 0.3162\n",
      "..........................End of print..................................\n",
      "\n",
      "Trimmed from 23768 pairs to 9319, 0.3921 of total\n"
     ]
    }
   ],
   "source": [
    "# trim words with frequency less than min_freq = 5 (set at vocab __init__) \n",
    "vocab.trim()\n",
    "\n",
    "# filter out pairs with trimmed words\n",
    "keep_pairs = list()\n",
    "for pair in pairs:\n",
    "    keep1, keep2 = True, True\n",
    "    #check question for trimmed words\n",
    "    for word in pair[0].split():\n",
    "        if word not in vocab.word2int:\n",
    "            keep1=False\n",
    "    #check answers for trimed words        \n",
    "    for word in pair[1].split():\n",
    "        if word not in vocab.word2int:\n",
    "            keep2=False\n",
    "\n",
    "    #pairs that do not contain trimmed words\n",
    "    if keep1 and keep2: keep_pairs.append(pair)\n",
    "print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[   1, 7996,  171, 2178,  543,  175,   18, 6150,  493,    9,    2],\n",
      "        [   1,   18, 5730,   35,  531, 1420,  171, 6345, 2039,    9,    2],\n",
      "        [   1,   44,   51, 2303,   32, 1116,  387,  349,  908,    9,    2],\n",
      "        [   1,   18,  289,   42,  924,   10, 2129, 2518,    9,    2,    0],\n",
      "        [   1,   10,   18,  174,   58, 4350, 4351,  720,    9,    2,    0]])\n",
      "lengths: tensor([11, 11, 11, 10, 10])\n",
      "target_variable: tensor([[   1, 2222, 5056,    2,    0],\n",
      "        [   1, 3641, 6345, 2039,    2],\n",
      "        [   1,  291,  470,    2,    0],\n",
      "        [   1, 1056,    2,    0,    0],\n",
      "        [   1, 3925,    2,    0,    0]])\n",
      "max_target_len: 5\n"
     ]
    }
   ],
   "source": [
    "#Build input and targets tensor\n",
    "from helper import pairs2TrainData\n",
    "batch_size = 5\n",
    "batches = pairs2TrainData([random.choice(pairs) for _ in range(batch_size)], vocab)\n",
    "input_variable, lengths, target_variable, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"max_target_len:\", max_target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#futher data preparation\n",
    "#train valid test 80-20\n",
    "random.shuffle(pairs)\n",
    "#20% validation data\n",
    "split = int(math.floor(len(pairs)*0.2))\n",
    "train_data, valid_data = pairs[split:], pairs[:split]\n",
    "#dict to both\n",
    "data = {'train':train_data, 'valid':valid_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor shape: torch.Size([512, 11])\n",
      "target_tensor shape: torch.Size([512, 6])\n"
     ]
    }
   ],
   "source": [
    "#Test Data Loaders\n",
    "from helper import data_loaders\n",
    "train_loaders = data_loaders(data=data['valid'], vocab=vocab, batch_size=512)\n",
    "input_tensor, lenght_tensor, target_tensor, max_target_len = next(train_loaders)\n",
    "print(\"input_tensor shape:\", input_tensor.shape)\n",
    "print(\"target_tensor shape:\", target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializet total 4336 embeddings with brown embedings, out of total 15173\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Total trainable parameters:2789293\n"
     ]
    }
   ],
   "source": [
    "from helper import initialize_embeddings\n",
    "from model import Seq2Seq\n",
    "#ALL TOGETHER:\n",
    "clip = 20\n",
    "teacher_forcing_ratio = 0.75\n",
    "lr = 0.1\n",
    "epochs = 5\n",
    "\n",
    "input_size = output_size = vocab.numb_words\n",
    "embedding_dim = 100\n",
    "hidden_size = 100\n",
    "n_layers = 2\n",
    "dropout = 0.2\n",
    "batch_size = 128\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')\n",
    "embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "initialize_embeddings(w2v,embeddings,vocab)\n",
    "\n",
    "model = Seq2Seq(\n",
    "    encoder_input_size=input_size,\n",
    "    encoder_hidden_size = hidden_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    decoder_hidden_size=hidden_size,\n",
    "    decoder_output_size=output_size\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "print(f'Total trainable parameters:{sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 59it [00:19,  3.09it/s]                                               \n",
      "Validation: 15it [00:01,  8.92it/s]                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4672562591100147\n",
      "3.6068306763966884\n",
      "Epoch: 1 \tTraining Loss: 3.467256 \tValidation Loss: 3.606831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 59it [01:12,  1.23s/it]                                               \n",
      "Validation: 15it [00:03,  4.83it/s]                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.883501501406653\n",
      "3.7161989847819012\n",
      "Epoch: 2 \tTraining Loss: 2.883502 \tValidation Loss: 3.716199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 59it [01:28,  1.50s/it]                                               \n",
      "Validation: 15it [00:01,  8.89it/s]                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.798107838226576\n",
      "3.403855641682943\n",
      "Epoch: 3 \tTraining Loss: 2.798108 \tValidation Loss: 3.403856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 59it [00:22,  2.61it/s]                                               \n",
      "Validation: 15it [00:01,  9.22it/s]                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7328448901742197\n",
      "3.4540433247884117\n",
      "Epoch: 4 \tTraining Loss: 2.732845 \tValidation Loss: 3.454043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 59it [01:08,  1.15s/it]                                               \n",
      "Validation: 15it [00:02,  6.19it/s]                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.680362733743958\n",
      "3.4418474674224853\n",
      "Epoch: 5 \tTraining Loss: 2.680363 \tValidation Loss: 3.441847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from trainer import optimize\n",
    "#optimize(\n",
    "    #data=data,\n",
    "    #model=model,\n",
    "    #optimizer=optimizer,\n",
    "    #criterion=criterion,\n",
    "    #n_epochs=epochs,\n",
    "    #save_path=os.path.join(os.getcwd(), 'save_model'),\n",
    "    #device=device,\n",
    "    #vocab=vocab,\n",
    "    #batch_size=batch_size,\n",
    "    #clip=clip,\n",
    "    #teacher_forcing=teacher_forcing_ratio,\n",
    "    #interactive_tracking=False    \n",
    "   # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after gpu training\n",
    "model.load_state_dict(torch.load('model_save_last', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import evaluateInput\n",
    "model.eval()\n",
    "#evaluateInput(model, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
