{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import print_data_for_visualization\n",
    "#print_data_for_visualization()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract questions and answers from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('When the CPJP continued to fight, what did other groups do?', None), ('What is one thing the FACA peace agreement called for when signed in April 2007?', None), ('Who became president in 2013?', 'Michel Djotodia'), ('What was Bozize indicted for?', 'crimes against humanity'), ('What mass murder did Bozize commit?', 'genocide'), ('How many people were displaced in the unrests?', '200,000'))\n"
     ]
    }
   ],
   "source": [
    "# RUN this cell only first time\n",
    "from helper import get_QA\n",
    "\n",
    "path = '.data/train-v2.0.json'\n",
    "#gets questions and answers from data\n",
    "questions, answers = get_QA(path=path)\n",
    "print(tuple(zip(questions[86815:86821], answers[86815:86821])))\n",
    "\n",
    "#export questions and answers to QA.txt\n",
    "delimiter ='\\t'\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "#output path for QA.txt\n",
    "data_path = os.path.join(os.getcwd(), '.data/QA.txt')\n",
    "#export QA pairs to QA.txt\n",
    "with open(data_path, 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in tuple(zip(questions, answers)):\n",
    "        #keep only pairs that have answers\n",
    "        if pair[1] is not None:\n",
    "            writer.writerow(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['in what r&b group was she the lead singer ?', \"destiny's child\"],\n",
       " ['what album made her a worldwide known artist ?', 'dangerously in love'],\n",
       " [\"who managed the destiny's child group ?\", 'mathew knowles'],\n",
       " ['when did beyonce rise to fame ?', 'late 1990s'],\n",
       " [\"what role did beyonce have in destiny's child ?\", 'lead singer']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper import normalizeString\n",
    "data_path = os.path.join(os.getcwd(), '.data/QA.txt')\n",
    "pairs = list()# list of QA lists\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        pairs.append([normalizeString(s) for s in line.split('\\t')])\n",
    "\n",
    "pairs[5:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking sequence lenght accros pairs\n",
    "max_len = list()\n",
    "for items in pairs:\n",
    "    for item in items:\n",
    "        max_len.append(len(item.split()))\n",
    "max_len.sort(reverse=True)\n",
    "#max_len[300:400] keep pairs with MAX_LENGTH up to 20 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 86821 sentence pairs\n",
      "Trimming pairs with sentence longer than MAX_LEN(20)......\n",
      "trimmed to 83988 pairs\n"
     ]
    }
   ],
   "source": [
    "from helper import filter_pairs\n",
    "#TRIMM PAIRS WITH MAX_LEN > 20\n",
    "MAX_LEN = 20\n",
    "pairs = filter_pairs(pairs, MAX_LEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populating vocabulary........\n",
      "Counted words: 69400\n"
     ]
    }
   ],
   "source": [
    "#Building Vocabulary\n",
    "from vocabulary import Vocab\n",
    "#initialize vocab object\n",
    "vocab = Vocab('SQuAD2.0')\n",
    "print('populating vocabulary........')\n",
    "for pair in pairs:\n",
    "    vocab.add_sentence(pair[0])\n",
    "    vocab.add_sentence(pair[1])\n",
    "print('Counted words: {}'.format(vocab.numb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........This is print from Voc class after call to trimm method.........\n",
      "keep_words 22939 / 69400 = 0.3305\n",
      "..........................End of print..................................\n",
      "\n",
      "Trimmed from 83988 pairs to 46199, 0.5501 of total\n"
     ]
    }
   ],
   "source": [
    "# trim words with frequency less than min_freq = 3\n",
    "vocab.trim()\n",
    "\n",
    "# filter out pairs with trimmed words\n",
    "keep_pairs = list()\n",
    "for pair in pairs:\n",
    "    keep1, keep2 = True, True\n",
    "    #check question for trimmed words\n",
    "    for word in pair[0].split():\n",
    "        if word not in vocab.word2int:\n",
    "            keep1=False\n",
    "    #check answers for trimed words        \n",
    "    for word in pair[1].split():\n",
    "        if word not in vocab.word2int:\n",
    "            keep2=False\n",
    "\n",
    "    #pairs that do not contain trimmed words\n",
    "    if keep1 and keep2: keep_pairs.append(pair)\n",
    "print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "\n",
    "pairs = keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[   1,    1,    1,    1,    1],\n",
      "        [  10,    3,   14,   14,   14],\n",
      "        [1965,    4, 1340,  117,  235],\n",
      "        [  14,   11,  117,   11,   70],\n",
      "        [ 197, 9266,   11,  566, 9668],\n",
      "        [   4,  449, 6991, 1494,   18],\n",
      "        [  11, 6054, 2670,   70, 8768],\n",
      "        [2639,  373, 6316, 2521,    9],\n",
      "        [2594,  754,   70,    9,    2],\n",
      "        [3068,  286,    9,    2,    0],\n",
      "        [ 197, 5558,    2,    0,    0],\n",
      "        [9482,    9,    0,    0,    0],\n",
      "        [ 180,    2,    0,    0,    0],\n",
      "        [   9,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([15, 13, 11, 10,  9])\n",
      "target_variable: tensor([[    1,     1,     1,     1,     1],\n",
      "        [ 1574,   868,    11,  6585, 17686],\n",
      "        [ 2594,   463,   755,  2521,     2],\n",
      "        [ 3068,  4691,  6991,     2,     0],\n",
      "        [  197,     2,  1340,     0,     0],\n",
      "        [    2,     0,     2,     0,     0]])\n",
      "max_target_len: 6\n"
     ]
    }
   ],
   "source": [
    "#Build input and targets tensor\n",
    "from helper import pairs2TrainData\n",
    "batch_size = 5\n",
    "batches = pairs2TrainData([random.choice(pairs) for _ in range(batch_size)], vocab)\n",
    "input_variable, lengths, target_variable, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"max_target_len:\", max_target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "torch.Size([6, 5])\n",
      "tensor([[19715, 17712,  4480,  4480, 21347],\n",
      "        [ 4298,  4480, 16284,  4480,  4480],\n",
      "        [17406, 17712,  4480, 17843, 16222],\n",
      "        [10829, 20345, 13556, 16464, 16222],\n",
      "        [10829, 17712,  6534,  7908, 17406],\n",
      "        [13605, 15847,  6534, 10997,  7739]])\n",
      "tensor([19715,  4298, 17406, 10829, 10829, 13605])\n",
      "['somerset,', 'satellite', '51st', 'ligands', 'ligands', 'asexual']\n"
     ]
    }
   ],
   "source": [
    "# initialize encoder, decoder and test forward pass trought seq2seq \n",
    "from helper import Seq2seq\n",
    "input_size = output_size = vocab.numb_words\n",
    "embedding_dim = 300\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "dropout = 0.25\n",
    "\n",
    "embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "\n",
    "seq2seq = Seq2seq(input_size, embeddings, embedding_dim, hidden_size, output_size, dropout, n_layers)\n",
    "a = seq2seq(input_variable, target_variable, max_target_len, lengths)\n",
    "print(a.shape)#seq_len, batch_size\n",
    "print(a)\n",
    "print(a[:,0]) # one data point from decoder output\n",
    "print([vocab.int2word[x.item()] for x in a[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26188486"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in seq2seq.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train valid test 70-20-10\n",
    "random.shuffle(pairs)\n",
    "#20% validation data\n",
    "split = int(math.floor(len(pairs)*0.2))\n",
    "train_data, valid_data = pairs[split:], pairs[:split]\n",
    "#10 test data\n",
    "split = int(math.floor(len(pairs)*0.1))\n",
    "train_data, test_data = train_data[split:], train_data[:split]\n",
    "#dict to hold all 3\n",
    "data = {'train':train_data, 'valid':valid_data, 'test':test_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor shape: torch.Size([21, 256])\n",
      "target_tensor shape: torch.Size([17, 256])\n"
     ]
    }
   ],
   "source": [
    "#Test Data Loaders\n",
    "from helper import data_loaders\n",
    "train_loaders = data_loaders(data=data['train'], vocab=vocab, batch_size=256)\n",
    "input_tensor, lenght_tensor, target_tensor, max_target_len = next(train_loaders)\n",
    "print(\"input_tensor shape:\", input_tensor.shape)\n",
    "print(\"target_tensor shape:\", target_tensor.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60ba2619b219c729b486742eb051bc9c57b9743288ab438bb8ff19641dfd240c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
